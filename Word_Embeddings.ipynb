{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDrSvjVSrkc+YS1Du/nNNB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brandonowens24/Word_Embeddings/blob/main/Word_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2nLVxP2jEPg"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "dataset = load_dataset(\"wikipedia\", \"20220301.simple\")\n",
        "documents = dataset['train']['text'][:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class my_LM(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_size, context_size=2, embs=None):\n",
        "        super(my_LM, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(vocab_size, emb_dim)\n",
        "        if embs:\n",
        "            self.embedding_layer = nn.Embedding.from_pretrained(embs)\n",
        "        self.linear1 = nn.Linear(emb_dim * context_size, hidden_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
        "        self.softmax = nn.Softmax(dim=0)\n",
        "\n",
        "def forward(self, x):\n",
        "    # flatten into a 1d output, concatenating vectors\n",
        "    # from each embedding in the input\n",
        "    x = torch.flatten(self.embedding_layer(x), start_dim=-2)\n",
        "    x = self.linear1(x)\n",
        "    x = self.sigmoid(x)\n",
        "    x = self.linear2(x)\n",
        "    x = self.softmax(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "aPUjONqLjmOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "def normalization(document, context_size=2):\n",
        "    doc_tokens = []\n",
        "    sentences = nltk.sent_tokenize(document)\n",
        "    for sentence in tqdm(sentences):\n",
        "        sent_tokens = nltk.word_tokenize(sentence)\n",
        "        sent_tokens = [word.lower() for word in sent_tokens if word]\n",
        "        doc_tokens += ['<s>']*context_size + sent_tokens + ['</s>']*context_size\n",
        "    return doc_tokens\n",
        "\n"
      ],
      "metadata": {
        "id": "b5y3N0QjjJlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bidict import bidict\n",
        "def create_context_vectors(documents, context_size=2):\n",
        "    token2id = bidict()\n",
        "    next_avail_token_id = 0\n",
        "    X = []\n",
        "    Y = []\n",
        "    for text in tqdm(documents):\n",
        "        tokens = normalization(text, context_size)\n",
        "        for i in range(len(tokens) - (context_size + 2)):\n",
        "            outside_tokens = tokens[i: i + context_size] + tokens[i + 1 + context_size: i + 1 + 2 * context_size ]\n",
        "            centered_token = tokens[i + context_size]\n",
        "            for token in outside_tokens + [centered_token]:\n",
        "                if token not in token2id:\n",
        "                    token2id[token] = next_avail_token_id\n",
        "                    next_avail_token_id += 1\n",
        "            x = [token2id[t] for t in outside_tokens]\n",
        "            y = token2id[centered_token]\n",
        "            X.append(x)\n",
        "            Y.append(y)\n",
        "    vocab_size = len(token2id)\n",
        "    return X,Y,token2id,vocab_size\n",
        "\n"
      ],
      "metadata": {
        "id": "toU5dyC7jLiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 2\n",
        "X,Y,token2id,vocab_size = create_context_vectors(documents, context_size)"
      ],
      "metadata": {
        "id": "-T-xNbP7jan7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}